# README â€“ Ã‰valuation de ModÃ¨les LLM  

## ğŸ“˜ PrÃ©sentation  

Ce dÃ©pÃ´t a pour ambition de fournir un cadre complet pour **Ã©valuer des modÃ¨les de langage**.  
Il regroupeâ€¯:  

- **Jeux dâ€™Ã©valuation** (datasets) prÃªts Ã  lâ€™emploi.  
- **Outillage** dâ€™Ã©valuation, notamment lâ€™intÃ©gration de **[Promptfoo](https://www.promptfoo.dev/)** pour la crÃ©ation, lâ€™exÃ©cution et le suivi de scÃ©narios de tests automatisÃ©s.  

Le projet vise Ã  simplifier la mise en place dâ€™un processus dâ€™Ã©valuation reproductible et extensible, adaptÃ© aux besoins des Ã©quipes du MinistÃ¨re de lâ€™IntÃ©rieur et de lâ€™administration territoriale de lâ€™Ã‰tat (ATE).

---

## ğŸš€ DÃ©marrage rapide  

### PrÃ©requis  

| PrÃ©requis | Version minimale |
|-----------|------------------|
| Node.js   | 18.x |
| npm / yarn| 9.x |
| Python    | 3.9 (si vous utilisez les scripts Python) |
| Docker (optionnel) | 20.10.x |

### Installation  

```bash
# 1. Cloner le dÃ©pÃ´t
git clone https://github.com/your-org/model-evaluation.git
cd model-evaluation

# 2. Installer les dÃ©pendances Node
npm install   # ou `yarn install`

# 3. Installer les dÃ©pendances Python (optionnel)
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Configuration de Promptfoo  

```bash
# Installer Promptfoo globalement (ou en tant que dÃ©pendance du projet)
npm install -g @promptfoo/cli

# VÃ©rifier lâ€™installation
promptfoo --version
```

---

## ğŸ“Š Jeux dâ€™Ã©valuation  

Le rÃ©pertoire `datasets/` contient plusieurs jeux de testsâ€¯:  

| Nom du dataset | Description | Format |
|----------------|-------------|--------|
| `qa_french.json` | Questionsâ€‘rÃ©ponses en franÃ§ais (exempleâ€¯: administration publique) | JSONL |
| `summarization.csv` | Paires texteâ€¯/â€¯rÃ©sumÃ© pour Ã©valuer la capacitÃ© de synthÃ¨se | CSV |
| `bias_test.yaml` | ScÃ©narios ciblant les biais de sensibilitÃ© | YAML |

> **Note**â€¯: chaque jeu est accompagnÃ© dâ€™un fichier `README.md` dÃ©taillant la provenance, la licence et les instructions dâ€™utilisation.

---

## ğŸ› ï¸ Utilisation de Promptfoo  

Promptfoo permet de **dÃ©crire, exÃ©cuter et comparer** des scÃ©narios de prompts.  

Exemple de fichier de configuration (`promptfoo.config.js`)â€¯:

```js
module.exports = {
  prompts: [
    {
      name: "QA FranÃ§ais",
      description: "Ã‰valuation de la pertinence des rÃ©ponses en franÃ§ais.",
      provider: "openai",
      model: "gpt-4o-mini",
      dataset: "datasets/qa_french.json",
      evaluator: "accuracy",
    },
    // â€¦autres scÃ©narios
  ],
};
```

Lancer les testsâ€¯:

```bash
promptfoo run
```

Les rÃ©sultats sont gÃ©nÃ©rÃ©s sous forme de rapports HTML et JSON dans le dossier `reports/`.

---

## ğŸ¤ Contribuer  

Les contributions sont les bienvenuesâ€¯!  

- **Issues** â€“ Signalez bugs, proposez des amÃ©liorations ou discutez de nouvelles idÃ©es.  
- **Pull Requests** â€“ Soumettez vos ajouts (nouveaux jeux de donnÃ©es, meilleures mÃ©triques, scripts dâ€™automatisation, etc.).  

> Veuillez respecter les bonnes pratiques suivantesâ€¯:  
> 1. CrÃ©ez une branche dÃ©diÃ©e (`feature/nom-de-la-fonction`).  
> 2. Ajoutez des tests unitaires et/ou dâ€™intÃ©gration lorsque cela est pertinent.  
> 3. Mettez Ã  jour la documentation (README, commentaires, etc.).  
> 4. Soumettez la PR en utilisant les **templates** fournis dans le rÃ©pertoire `.github/`.  

Toutes les contributions seront revues dans les plus brefs dÃ©lais.

---

## ğŸ“„ Licence  

Ce projet est publiÃ© sous licence **Apacheâ€¯2.0**. Consultez le fichier `LICENSE` pour plus de dÃ©tails.

---

## ğŸ“ Contact  

- **Mainteneur principal**â€¯: [Fabrique NumÃ©rique MinistÃ¨re de l'IntÃ©rieur]  
- **Canal de discussion**â€¯: Utilisez les **Discussions GitHub** du dÃ©pÃ´t pour poser des questions ou proposer des idÃ©es.  

---

> **Rappel**â€¯: Lâ€™utilisateur reste responsable de la vÃ©rification finale et de lâ€™usage des contenus gÃ©nÃ©rÃ©s. Cette solution doit Ãªtre employÃ©e conformÃ©ment aux rÃ¨gles de l'Etat et aux exigences du RGPD.
